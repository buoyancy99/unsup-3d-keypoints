<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<link href="res/css/bootstrap.min.css" rel="stylesheet">

<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 100%;
  }

  h1 {
    font-weight:300;
    max-width: 100%;
  }

  div {
    max-width: 95%;
    margin:auto;
    padding: 10px;
  }

  .table-like {
    display: flex;
    flex-wrap: wrap;
    flex-flow: row wrap;
    justify-content: center;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img {
    padding: 0;
    display: block;
    margin: 0 auto;
    max-height: 100%;
    max-width: 100%;
  }

  table {
    padding: 0;
    display: block;
    margin: 0 auto;
    max-height: 100%;
    max-width: 100%;
  }

  iframe {
    max-width: 100%;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    max-width: 1100px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }

  .no-gutters {
    margin-right: 0;
    margin-left: 0;
  }

  .column {
    float: center;
    width: 50%;
    padding: 5px;
  }

  /* Clearfix (clear floats) */
  .row::after {
    content: "";
    clear: both;
    display: table;
  }
</style>
<!-- ======================================================================= -->

<!-- Start : Google Analytics Code -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-64069893-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-64069893-4');
</script> -->
<!-- End : Google Analytics Code -->

<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
<div max-width=100%>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="icon" type="image/png" href="resources/thumbnail.png" />
  <title>Unsupervised Learning of Visual 3D Keypoints for Sensorimotor Control</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="canonical" href="https://buoyancy99.github.io/unsup-3d-keypoints/" />
  <meta name="referrer" content="no-referrer-when-downgrade" />

  <meta property="og:site_name" content="" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="Unsupervised Learning of Visual 3D Keypoints for Control" />
  <meta property="og:description" content="Chen, Abbeel, Pathak. Unsupervised Learning of Visual 3D Keypoints for Control. ICML 2021." />
  <meta property="og:url" content="https://buoyancy99.github.io/unsup-3d-keypoints/" />
  <meta property="og:image" content="https://buoyancy99.github.io/unsup-3d-keypoints/resources/teaser-website.png" />
  <meta property="og:video" content="https://www.youtube.com/v/Dcmb8qQWxBE" />

  <meta property="article:publisher" content="https://github.com/buoyancy99" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Unsupervised Learning of Visual 3D Keypoints for Control" />
  <meta name="twitter:description" content="Chen, Abbeel, Pathak. Unsupervised Learning of Visual 3D Keypoints for Control. ICML 2021." />
  <meta name="twitter:url" content="https://buoyancy99.github.io/unsup-3d-keypoints/" />
  <meta name="twitter:image" content="https://buoyancy99.github.io/unsup-3d-keypoints/resources/teaser-website.png" />
  <!-- <meta name="twitter:label1" content="Written by" />
  <meta name="twitter:data1" content="Deepak Pathak" /> -->
  <!-- <meta name="twitter:label2" content="Filed under" />
  <meta name="twitter:data2" content="" /> -->
  <meta name="twitter:site" content="@pathak2206" />
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

  <script src="https://www.youtube.com/iframe_api"></script>
  <meta name="twitter:card" content="player" />
  <meta name="twitter:image" content="https://buoyancy99.github.io/unsup-3d-keypoints/resources/teaser-website.png" />
  <meta name="twitter:player" content="https://www.youtube.com/embed/Dcmb8qQWxBE" />
  <meta name="twitter:player:width" content="640" />
  <meta name="twitter:player:height" content="360" />
</head>

<body>
      <br>
      <center><span style="font-size:44px;font-weight:bold;">Unsupervised Learning of Visual 3D Keypoints for Control</span></center><br/>
      <div class="table-like" style="justify-content:space-evenly;max-width:900px;margin:auto;">
          <div><center><span style="font-size:18px"><a href="https://boyuan.space/" target="_blank">Boyuan Chen</a></span></center>
          <center><span style="font-size:18px">UC Berkeley</span></center>
          </div>

          <div><center><span style="font-size:18px"><a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a></span></center>
          <center><span style="font-size:18px">UC Berkeley</span></center>
          </div>

          <div><center><span style="font-size:18px"><a href="https://www.cs.cmu.edu/~dpathak/" target="_blank">Deepak Pathak</a></span></center>
          <center><span style="font-size:18px">CMU</span></center>
          </div>
      </div>

      <center><span style="font-size:20px;"><a href='https://icml.cc/Conferences/2021'>International Conference on Machine Learning (ICML), 2021</a></span></center>

      <div class="table-like" style="justify-content:space-evenly;max-width:700px;margin:auto;padding:5px">
        <div><center><span style="font-size:28px"><a href="https://arxiv.org/abs/2106.07643">[Paper]</a></span></center></div>
        <div><center><span style="font-size:28px"><a href='https://github.com/buoyancy99/unsup-3d-keypoints'>[GitHub Code]</a></span></center> </div>
        <div><center><span style="font-size:28px"><a href='https://youtu.be/XnRzzxnlMOM'>[Oral Talk]</a></span></center> </div>
        <!-- <div><center><span style="font-size:28px"><a href='https://youtu.be/TODO' target="_blank">[Long Talk]</a></span></center> </div> -->
      </div>

      <center>
        <iframe width="720" height="480" src="https://www.youtube.com/embed/Dcmb8qQWxBE?rel=0&controls=0&showinfo=0&autoplay=1&loop=1&autohide=1&modestbranding=1&playlist=Dcmb8qQWxBE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <br><hr>

      <center><h1>Abstract</h1></center>
      <div style="width:960px; margin:0 auto; text-align: justify">
        Learning sensorimotor control policies from high-dimensional images crucially relies on the quality of the underlying visual representations. Prior works show that structured latent space such as visual keypoints often outperforms unstructured representations for robotic control. However, most of these representations, whether structured or unstructured are learned in a 2D space even though the control tasks are usually performed in a 3D environment. In this work, we propose a framework to learn such a 3D geometric structure directly from images in an end-to-end unsupervised manner. The input images are embedded into latent 3D keypoints via a differentiable encoder which is trained to optimize both a multi-view consistency loss and downstream task objective. These discovered 3D keypoints tend to meaningfully capture robot joints as well as object movements in a consistent manner across both time and 3D space. The proposed approach outperforms prior state-of-art methods across a variety of reinforcement learning benchmarks.
      </div>
      <br><hr>

      <center><h1>Main Idea</h1></center>
      <div style="width:960px; margin:0 auto; text-align: justify">
        In a 3D scene, we collect observation from multiple cameras. Our unsupervised learning method learns temporally-consistent 3d keypoints via interaction with the environment. We jointly train a policy on top of the learned 3d keypoints. The keypoint based policy enables more efficient task learning.
      </div>
      <center><a href="resources/teaser-website.png"><img src = "resources/teaser-website.png" width="600px" ></img></a><br></center>
      <hr>


      <center><h1>Overview of the Algorithm</h1></center><br/>
      <center><a href="resources/method.png"><img src = "resources/method.png" width="800px"></img></a><br></center>
      <div style="width:960px; margin:0 auto; text-align: justify">
        (a) For each camera view, a fully convolutional neural network encodes the input image into K heat maps and depth maps. (b) We then treat these heat maps as probabilities to compute expectation of spatial uv coordinates in camera plane. These expected values and the saptial variances are used to resample final uv keypoint coordinates which adds noise that prevents the decoder from cheating to hide the input information in the relative locations uv keypoints. We also take expectation of depth coordinate, d, using the same probability distribution. These [u,v,d] coordinates are then unprojected into the world coordinate. (c) We take attention-weighted average of keypoint estimations from different camera views to get a single prediction in the world coordinate. (d) For decoding, we project predicted keypoints in world coordinate to [u, v, d] in each camera plane. (e) Each keypoint coordinate is mapped to a gaussian map, where a 2D gussian is created with mean at [u, v] and std inversely proportional to d. For each camera, gaussian maps are stacked together and passed into decoder to reconstruct observed pixels from the camera. (f) Together with reconstruction, we also jointly train a task MLP policy on top of predicted world coordinates via reinforcement learning.
      </div>
      <br/><hr>

      <center><h1>Visualizations</h1></center>
      The following figures visualize the discovered 3d keypoints reprojected onto the camera planes.<br/>
      <a href="resources/metaworld.gif"><img src = "resources/metaworld.gif" width="650px" ></img></a><br>
      <a href="resources/bc_visualization.png"><img src = "resources/bc_visualization.png" width="800px" ></img></a><br>
      <a href="resources/hammer_visualization.png"><img src = "resources/hammer_visualization.png" width="800px" ></img></a><br>
      <hr><br/>
      <a href="resources/ant.gif"><img src = "resources/ant.gif" width="500px" ></img></a><br>
      <a href="resources/ant_visualization.png"><img src = "resources/ant_visualization.png" width="800px" ></img></a><br>
      <hr>
      The following video shows our policy successfully dressing a human with a scarf.<br/><br/>
      <iframe width="400" height="400" src="https://www.youtube.com/embed/Axt3UMgFV7s?rel=0&autohide=1&showinfo=0&autoplay=1&loop=1&modestbranding=1&playlist=Axt3UMgFV7s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <br/><br/><hr>

      <center id="oralTalk"><h1>ICML Oral Presentation</h1></center>
      <iframe width="720" height="480" src="https://www.youtube.com/embed/XnRzzxnlMOM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <br><br><hr>

      <center><h1>Paper and Bibtex</h1></center>
      <table align=center width=850px>
      <tr>
      <td width="30%" align=left>
      <!-- <p style="margin-top:4px;"></p> -->
      <a href="https://arxiv.org/pdf/2106.07643"><img style="height:150px" src="resources/thumbnail.png"/></a>
      <center>
      <span style="font-size:20pt"><a href="https://arxiv.org/pdf/2106.07643">[Paper]</a></span>
      <span style="font-size:20pt"><a href="https://arxiv.org/abs/2106.07643">[ArXiv]</a></span>
      <span style="font-size:20pt"><a href='https://github.com/buoyancy99/unsup-3d-keypoints'>[GitHub]</a></span>
      </center>
      </td>
      <td width="6%" align=center>
      </td>
      <td width="64%" align=left>
      <!-- <p style="margin-top:4px;"></p> -->
      <p style="text-align:left;"><b><span style="font-size:20pt">Citation</span></b><br/><span style="font-size:6px;">&nbsp;<br/></span> <span style="font-size:15pt">Boyuan Chen, Pieter Abbeel, Deepak Pathak. <b>Unsupervised Learning of Visual 3D Keypoints for Control.</b> ICML 2021.</span></p>
      <!-- <p style="margin-top:20px;"></p> -->
      <span style="font-size:20pt"><a shape="rect" href="javascript:togglebib('paper_bib')" class="togglebib">[Bibtex]</a></span>
      </td>
      </tr>
      <tr>
      <td width="30%" align=left>
      </td>
      <td width="6%" align=center>
      </td>
      <td width="64%" align=left>
        <div class="paper" id="paper_bib">
<pre xml:space="preserve">
@inproceedings{chen2021unsupervised,
  title={Unsupervised learning of visual 3d keypoints for control},
  author={Chen, Boyuan and Abbeel, Pieter and Pathak, Deepak},
  booktitle={International Conference on Machine Learning},
  pages={1539--1549},
  year={2021},
  organization={PMLR}
}
</pre>
                </div>
                </td>
                </tr>
            </table>
          <hr>
      </table>
    
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

</div>
</body>
</html>
